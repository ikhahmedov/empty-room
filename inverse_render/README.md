Inverse Rendering
=================

This folder contains the code to perform inverse rendering and architectural
analysis on an indoor scene in order to extract the parameters of a room
model.

Input:
* A set of images of the room
* A .ply mesh of the room
* A .cam file specifying the images and the associated camera positions
* Optionally, a set of depth images (.pcd) for each image
* Optionally, a set of confidence maps (.conf) for each image

Building
--------
    $ mkdir build
    $ cd build
    $ cmake ..
    $ make

Running
-------
Navigate to the folder containing your input data. At minimum, to compute
everything run

    $ invrender model.ply -camfile cameras.cam

I recommend saving and using intermediate files to save computation time.
There are three primary intermediate computations that can be saved:
* Wallfinding results, including the normalization transform, floor plan,
floor and ceiling level, and labels for each mesh vertex.
* Reprojection results, including a list of image samples for each vertex
in the mesh.
* Point environment samples, containing the computed incoming indirect
and direct lighting coefficients for a number of randomly-sampled wall
vertices.

There is a relatively complete list of command line flags included:

    $ invrender

Other intermediates can be generated by the program, such as oriented
edge detection filter results.

Source Code Overview
--------------------

### Important Data Structures ###
* Mesh (mesh.h/.cpp): Contains all the info about the mesh. This includes:
  * Mesh geometry (pointer to an R3Mesh)
  * Per-vertex data field "types", used to store architectural
    label (wall, floor, ceiling)
  * Per-vertex data field "labels", used to store lighting label
    (id of an area light). 
  * Per-vertex list of reprojected samples (see below)
  * Also contains OpenGL helper methods to render the mesh, used
    in hemicuberenderer
* Sample (mesh.h/.cpp): We add a sample to a mesh vertex every time part of
  an image pixel projects onto that vertex.
  * Color of the pixel,
  * Confidence of the pixel (if applicable)
  * Direction to the camera from the vertex
  * Form factor from camera to vertex. If the vertex is far from the camera or 
    at an angle from the camera, then the relative proportion of the pixel taken
    up by the vertex is smaller than it would otherwise be.
* CameraParams (colorhelper.h/.cpp): Stores the parameters for a single camera
  * Position
  * Orientation
  * Field of view in degrees/focal length in pixels (in the y-direction)
  * Dimensions of image
* ColorHelper (colorhelper.h/.cpp): Stores all the 2D data related to each image,
  i.e. camera position
  * HDR (32-bit float) color imagery
  * Confidence maps (e.g. underexposed and overexposed pixels have low confidence),    if applicable.
  * Depth maps, if applicable
  * Label images denoting if a pixel belongs to a floor, wall, etc.
  * Edge images, denoting the response to an edge detection filter oriented
    along the mesh's coordinate axes
  * Camera parameters for each image
  * Also contains read/write functions for each file type.
* SampleData (hemicuberenderer.h/.cpp): One SampleData structure represents
  the known quantities in the equation B_i = p(sum(B_jF_ij) + sum(L_kF_ik) for
  a single point i.
  * B_i is radiosity
  * sum(B_jF_ij) is netIncoming
  * F_ik is lightamount[k]
  * i is vertexid
  * fractionUnknown denotes how much of the hemisphere visible from the point
    does not have any sample data; thus if fractionUnknown is high we want to
    place low importance on this equation in our optimization.
* HemicubeRenderer (hemicuberenderer.h/.cpp): A helper that performs tasks
  related to rendering the mesh.The basic operation is to render the mesh
  from a specified set of camera parameters. This can either give HDR color
  images, or label images. The HemicubeRenderer can also compute the SampleData
  for a given vertex by rendering a hemicube
* Texture (material.h): Specification of a texture (e.g. carpet)
  * A square 32-bit float texture containing the image
  * Number of pixels on each side of the texture image
  * Texture scale - how big in world units one pixel of the texture image is.
* Material (material.h): Currently consists of a diffuse color (three 32-bit
  channels) and an optional texture.

Also important to be familiar with the GAPS geometry primitives (R3Mesh,
R3Vector, R4Matrix, etc.) and Eigen linear algebra primitives (Vector3f, etc.).

### Algorithms ###
* Wallfinding (wallfinder.h/.cpp, orientation_finder.h/.cpp): Finds a manhattan
  world coordinate system and then identifies mutually perpendicular walls
  forming a closed floor plan, as well as floor and ceiling levels.
* Reprojection (reproject.h/.cpp): Reprojects HDR images onto scanned geometry
* Light identification (clusterlights.h/.cpp): Thresholds reprojected samples
  and identifies clusters of very bright vertices as area lights.
* Inverse Rendering (solver.h/.cpp, solvetexture.cpp): Uses hemicuberenderer
  to compute optimal light intensities and wall diffuse reflectance. Also
  determines a floor texture.
* Door/Window finding (linefinder.h/.cpp): Finds lines in HDR images that are
  oriented along the axes of the coordinate system using vanishing points.

### Application Code ###
* invrender.cpp is the main file that runs all the steps
* display.h/.cpp deals with the debugging visualization
* parseargs.h/.cpp manages command-line arguments

### Helper files ###
* Simplify.h - Mesh decimation algorithm
* rerender.h/.cpp - outputs computed geometry (walls, floor, ceiling) with
  appropriate diffuse reflectance, and lights, in various renderable formats
  such as RADIANCE and pbrt [outdated]
* eigen_nnls.h - Nonnegative least squares solver used in optimization
* shortestpath.h/.cpp - BFS shortest path/cycle algorithm used in wallfinding
* rgbe.h/.cpp - HDR format encoding/decoding

